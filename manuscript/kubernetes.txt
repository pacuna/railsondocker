# Kubernetes

## Introduction

Kubernetes has several components you can use to build your architecture.
We aren't going to go deep in all of these objects. I'll give a brief
overview of the most important ones that you need to know.
Also, Kubernetes is still in heavy development, so new features and new objects
appear with every big release.

The objects we are going to see are Pods, Jobs, Volumes, Replica Sets, Services and Deployments.
This are the basic elements to build almost any application. Then you can use
the other objects in case you need more specific features like keeping secrets
in your cluster, having certain network policies, or having different namespaces.

## Concepts

### Pods

The Pod is the first level of abstraction in Kubernetes. It's just one level above
the container. The difference is you can have several container running in the same Pod.
In general, is not recommended to do  this unless it's a very specific case where it actually makes sense. Normally
you want to have one container per Pod.
For our application, we'll use different Pods for our web app and for PostgreSQL.

An example template for a Pod for our application is:

    apiVersion: v1
    kind: Pod
    metadata:
      name: webapp
      labels:
        name: webapp
    spec:
      containers:
      - image: pacuna/webapp:d15587c
        name: webapp
        env:
        - name: PASSENGER_APP_ENV
          value: production
        ports:
        - containerPort: 80
          name: webapp
        imagePullPolicy: Always

As you can see, we always start by declaring the API version we want to use.
Then we declare the type of object. The good thing about declaring the type in the 
template, is that then you can run any template with the same command, and Kubernetes
will figure out what type of object you're creating.
The labels will help the Replica Set to identify which Pods should be associate with.
All the Kubernetes architecture is based on metadata. A Pod will have some metadata that will
allow it to be related to a Replica Set. Then the Replica Set will have metadata that will allow 
a Service to route requests to it. Finally, a Deployment will also know which Replica Sets
are associated to it via metadata.

The specifications section is pretty similar to a Docker Compose template.
We declare the image we need, environmental variables, ports we want to expose and extra
information. In this case we're adding an `imagePullPolicy` of always, so the container
is always pulled.

### Replica Set

This object manages our desired number of replicas for a Pod.
For example we can specify that we need at least 3 replicas for certain Pod. Then
the Replica Set will make sure that these 3 replicas are always up. If one Pod
dies for any reason, the Replica Set will create a new one.

An example Replica Set for our application is:

    apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      replicas: 3
      template:
        metadata:
          labels:
            app: webapp
            tier: frontend
        spec:
          containers:
          - image: pacuna/webapp:d15587c
            name: webapp
            env:
            - name: PASSENGER_APP_ENV
              value: production
            ports:
            - containerPort: 80
              name: webapp
            imagePullPolicy: Always


The Replica Set also uses metadata in order to identify the Pods in the set.
As you can see, you can add the Pod declaration inside of the Replica Set
specification. This type of embed is typical in Kubernetes configurations and can
help you to avoid having too many files that you need to update when a new
image is available for your app. Later you'll see that with just one file we can
have an entire system to launch.
An important part of the specification of a Replica Set is the `replicas` property.
As you can imagine, this number controls the number of Pods for this Set.

### Jobs

A Job is simply a Pod with a well defined task to accomplish. The action that
a Job does better than a Pod has to do with task completion. 
Every time you launch a Job, if for some reason the task fails, the Job
will stop and it'll start again until the task finishes. If you use a regular
Pod to run your task, you won't be sure your task gets completed, because if the Pod
dies, it won't start again.
Jobs are super effective for tasks such as fetching data from external sources,
long data processing tasks, long computing tasks, etc. But they are also useful
to keep separate our application maintenance tasks and our actual web application containers.
For example, in a typical Rails application we need to run migration and precompile assets
before a new deploy. Instead of running these tasks in the same web app container, we can run them
by launching independent Jobs and that way avoid collisions in the case we launch several
web application Pods.

This is an example for a Job that runs migrations for our web application:

    apiVersion: batch/v1
    kind: Job
    metadata:
      name: setup
    spec:
      template:
        metadata:
          name: setup
        spec:
          containers:
          - name: setup
            image: pacuna/webapp:ec4421
            command: ["./bin/rails",  "db:migrate", "RAILS_ENV=production"]
            env:
            - name: PASSENGER_APP_ENV
              value: production
          restartPolicy: Never

The specification is pretty similar to a Pod specification. We have to use the same
image that we have for our application, since it need to be up-to-date with the latest
migrations and also needs the configuration for writing to the database.
Like I mentioned before, this Job will run until it completes the task. After completion it won't
never be restarted until you run it again. We make sure of that by adding a `restartPolicy` of Never.

### Volumes

This object is pretty similar to what we know as Volumes in Docker.
It allows us add a persistent object to our Pods. The big advantage that Kubernetes has
is that it has a tight integration with cloud providers like AWS and GCE. This integration
makes possible the use of storage units like EBS in the case of AWS.
That's very important, since in some cases, simple volume mounts on the same host is not
enough. For example, let's say you have a Database running in a container, and you want
to mount the data folder so your data is persisted on the host. What happens if you run
a new container version of your Database and it gets deployed on a new node? Then
the volume mount won't work, because the data is persisted on another machine.
On the other side, if you mount a volume using something like EBS, then it's always
going to be that same external storage unit and you won't have any issues if your
container launches on different machines every time.

For example, this is the specification for a PostgreSQL container with an associated
EBS Volume that was created previously:

    spec:
      containers:
      - image: postgres:9.5.3
        name: postgres
        env:
        - name: POSTGRES_PASSWORD
          value: mysecretpassword
        - name: POSTGRES_USER
          value: webapp
        - name: POSTGRES_DB
          value: webapp_production
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
          name: postgres
        volumeMounts:
          - name: postgres-persistent-storage
            mountPath: /var/lib/postgresql/data
      volumes:
        - name: postgres-persistent-storage
          awsElasticBlockStore:
            volumeID: vol-fe268f4a
            fsType: ext4

The relevant part is:

        volumeMounts:
          - name: postgres-persistent-storage
            mountPath: /var/lib/postgresql/data
    volumes:
    - name: postgres-persistent-storage
      awsElasticBlockStore:
        volumeID: vol-fe268f4a
        fsType: ext4

The `volumes` part declares the Volume we want to use. In this case we use the ID
that AWS assigned to that EBS. Then inside the Pod's specification, we can add
a `volumeMounts` section indicating the name of the volume we want to use, and the path
that we want to be persisted.

### Services

A service is going to give us a static end point for our Replica Sets and Pods.
Every time you launch a new version of your Pods, their IP address will change.
It doesn't matter if it is internal or external, we need a mechanism to route
requests to those Pods.
A service can be a typical Load Balancer, or just declare that we want to expose
a port in our cluster that routes requests to the same group of Pods.
Every container orchestration framework has a mechanism to handle this issue. It's
necessary if you want to have a public end point for your application.

Kubernetes also has a nice integration with AWS ELBs. You can declare that your
service should be a Load Balancer and Kubernetes will create and associate one automatically.

This is an example for a Load Balancer service for our web app:

    apiVersion: v1
    kind: Service
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      ports:
        - port: 80
      selector:
        app: webapp
        tier: frontend
      type: LoadBalancer

Pretty simple, besides de metadata, we indicate we want to expose the port 80
and the type should be a Load Balancer. Like I said before, Kubernetes
will create a new ELB in our AWS account and it'll associated to the Pods
that match the metadata with this service.

### Deployments

Deployments were introduced in one of the latest Kubernetes releases. It can
be a little bit confusing at first, but when you start to using them, you see their value
and how they can make your update process to run very smoothly.
In a Deployment declaration, you can have your Replica Set and Pod specifications
declared together. The deployment will be in charge of running updates to the state
of these elements. So you can update the image in your Deployment, and the 
respective Replication Set and Pods will be updated as well.

Let's see an example deployment for PostgreSQL:

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: postgres
      labels:
        app: webapp
    spec:
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            app: webapp
            tier: postgres
        spec:
          containers:
          - image: postgres:9.5.3
            name: postgres
            env:
            - name: POSTGRES_PASSWORD
              value: mysecretpassword
            - name: POSTGRES_USER
              value: webapp
            - name: POSTGRES_DB
              value: webapp_production
            ports:
            - containerPort: 5432
              name: postgres

This Deployment template contains a Replica Set and Pod in its specification.
With this individual file, you can have a Pod, Replica Set and run updates
to both with only one command. We have to add a couple of extra properties, like
the strategy to run updates, which in this case should be Recreate since we don't want to have
more than one of this Pod running at the same time, but you can also use `RollingUpdate` which
will do the opposite.

Using Deployments it's recommended over using Replication Sets and Pods independently.

## Structuring the files

In order to keep a better application structure, let's add
some folders for the Kubernetes configuration.

We'll use a `kube` folder, and subfolders for every Kubernetes type we need.
For now, let's create a deployments and a jobs subfolder:

    $ mkdir -p kube
    $ mkdir -p kube/deployments
    $ mkdir -p kube/jobs

## Templates

### PostgreSQL

Let's create a deployment file for the PostgreSQL service:

    $ touch kube/deployments/postgres-deployment.yaml

And add the following:

      apiVersion: v1
      kind: Service
      metadata:
        name: postgres
        labels:
          app: webapp
      spec:
        ports:
          - port: 5432
        selector:
          app: webapp
          tier: postgres
        clusterIP: None
      ---
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: postgres
        labels:
          app: webapp
      spec:
        template:
          metadata:
            labels:
              app: webapp
              tier: postgres
          spec:
            containers:
            - image: postgres:9.5.3
              name: postgres
              env:
              - name: POSTGRES_PASSWORD
                value: mysecretpassword
              - name: POSTGRES_USER
                value: webapp
              - name: POSTGRES_DB
                value: webapp_development
              ports:
              - containerPort: 5432
                name: postgres

That's kind of a long YAML file (and it'll get longer), but keep in mind
that the file contains all the necessary elements that we need for the PostgreSQL
deployment. It'll generate the Pod, the replica set that's going to manage the Pod
and also the service so our web application and the setup container can reach
it by using the same alias we used in development with Docker Compose.

Let's go section by section:

    apiVersion: v1
    kind: Service
    metadata:
    name: postgres
    labels:
      app: webapp
    spec:
    ports:
      - port: 5432
    selector:
      app: webapp
      tier: postgres
    clusterIP: None

The first section of the file is the service declaration. In the deployment file
we can declare more than one kind, and we separate them by using `---`.

This service has a name `postgres`, meaning that we are going to be able to use
that alias for communicating with it from our cluster. Kubernetes uses its own
DNS mechanism to allow communication between services running on different nodes by using their
aliases.
We are also defining a label that can be useful for doing filtering.

The spec section will tell the service where it should be routing
the requests. In this case it's going to match two selectors: `app` and `tier`
with the values `webapp`, which is the main context, and the specific tier which
is `postgres`.
The `tier` is the selector that's going to differentiate the
web application containers and the PostgreSQL container, since both are going
to have an `app` selector with the value `webapp` but a different `tier` selector
value. The purpose of the `app` selector is to able to have several
different applications running in the cluster, and for example if you have another application
named `users`, you can have the same `postgres` tier but a different `app`
label.

We are also indicating the communication port should be 5432, which is the standard
for PostgreSQL. And finally we are telling Kubernetes that this service should be
only accessed from inside the cluster by using `clusterIP` setted to `None`. That means
you won't be able to hit this endpoint from outside the cluster.

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: postgres
      labels:
        app: webapp
    spec:
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            app: webapp
            tier: postgres
        spec:
          containers:
          - image: postgres:9.5.3
            name: postgres
            env:
            - name: POSTGRES_PASSWORD
              value: mysecretpassword
            - name: POSTGRES_USER
              value: webapp
            - name: POSTGRES_DB
              value: webapp_development
            ports:
            - containerPort: 5432
              name: postgres

The second section corresponds to the deployment itself.

We start by adding the same metadata we used for the service and then the
specifications.
Then, for the template of the specification, we have to specify the match we need for this
replica set and Pods. As I mentioned before, the service will be looking
for two labels: `app` and `tier` and it must match this metadata. We also
indicate we want to use a `Recreate` strategy type. This means that we want to kill
all the existing Pods for this deployment before launching new ones. If you don't specify 
a strategy, the deployment will use a default which is `RollingUpdate`. That type
won't killed the old Pods before launching the new ones, so you can control
your update process in a better way. That works good with web applications, but you
can see why that would be a problem when using containers for something like Databases.
You don't want to have two copies of the same Database container running at the same time.

Finally we define the specification for the containers. This is pretty
similar to the Docker Compose structure and it contains the image, the
environmental variables, the port and the container name.
Notice that we're still using the variables for development. Later we'll switch this
to production once we configured our application to run on that environment.

And that's it for the PostgreSQL deployment for now. Later we'll add another
section for the persistence storage. We are skipping that for now because
we first want to test this templates with a local tool, and the way you manage
volumes with Kubernetes, doesn't allow us to test that locally.

### Setup Container

In the case of the setup container, we don't need a replica set, service nor
deployment. We only need a Pod that gets the job done. For that we are going to use
a Kubernetes [Job](http://kubernetes.io/docs/user-guide/jobs/). Think of a Job
as a special kind of Pod that was meant to be use for these kinds of tasks.
It's going to create a Pod that'll run the command we indicate and it'll end
after the container finishes.

The syntax for writing a Job template is quite simple. Let's create the file:

    $ touch kube/jobs/setup-job.yaml

And add the following configuration, replacing the image with the one
you pushed to DockerHub. Remember we are using the latest commit hash for
tagging:

    apiVersion: batch/v1
    kind: Job
    metadata:
      name: setup
    spec:
      template:
        metadata:
          name: setup
        spec:
          containers:
          - name: setup
            image: pacuna/webapp:57514be
            command: ["/bin/bash", "./setup.sh"]
            env:
            - name: PASSENGER_APP_ENV
              value: development
          restartPolicy: Never

This Pod has a very simple definition. It has a `restartPolicy` of never, so
once it finishes it doesn't get restarted by the scheduler.

The `containers` section contains the specs for the image that we just pushed
to DockerHub.

Similar to what we did with Docker Compose, we set the necessary environmental variables,
and a command instruction to run our custom entrypoint.
As you can see, in Kubernetes there's not an `entrypoint` instruction but a `command` key
that behaves in a similar fashion. Remember that our entrypoint for this container should
be our setup.sh file that runs the migrations once the PostgreSQL container is ready.

### Web Application

This deployment file is also going to have two kinds, a service and a deployment.

    $ touch kube/deployments/webapp-deployment.yaml

And add the following template:

    apiVersion: v1
    kind: Service
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      ports:
        - port: 80
      selector:
        app: webapp
        tier: frontend
      type: NodePort
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      replicas: 3
      template:
        metadata:
          labels:
            app: webapp
            tier: frontend
        spec:
          containers:
          - image: pacuna/webapp:57514be
            name: webapp
            env:
            - name: PASSENGER_APP_ENV
              value: development
            ports:
            - containerPort: 80
              name: webapp
            imagePullPolicy: Always

Again, let's go by part. First, the service definition is pretty similar to the one
we have for the PostgreSQL deployment. The difference here is we are using a new set of
selectors. The PostgreSQL uses a tier named `postgres`, and this one uses a tier named
`frontend`. This way the service can route the requests to the correct replica set
that has those same labels.

Another difference is we are using a `type` for the service. Locally we can use
the type `NodePort`. Which is a way of exposing a port for accessing the application
from outside the cluster. On production we'll use a `LoadBalancer` type, type that
we don't have for our local cluster. The difference between these two types is that
the `NodePort` will expose a random port from the cluster for accessing the application.
This port can change between deploys. On the other side, the `LoadBalancer` type, will
use the provider's api (AWS or GCE) for creating a real Load Balancer in your account.
And this Load Balancer will have a static DNS we can use for connecting to the service, and also
will balance the requests among all the Pod replicas we declare in the specs.

Then we have the deployment section. This section contains the metadata with
the respective name and label and the specification section. We are indicating we want to have
3 Pods for this deployment. That means there are going to be 3 containers running our application, and
this service will be routing the traffic to those three Pods. We then declare the labels so the service
can locale the respective replica set.

Finally we have the container specifications. We are using the image we pushed to
DockerHub (replace with yours), passing the `PASSENGER_APP_ENV` variable
for setting the Rails environment, exposing the port and adding an `imagePullPolicy`
in order to always force the pull.

Your final structure for the deployment files should be something like:

    kube
    ├── deployments
    │   ├── postgres-deployment.yaml
    │   └── webapp-deployment.yaml
    └── jobs
        └── setup-job.yaml

Now it's time to try those templates on a local cluster. For that we are going
to use a very handy tool called Minikube.

## Minikube

According to the [official Minikube repository](https://github.com/kubernetes/minikube):

> Minikube is a tool that makes it easy to run Kubernetes locally.
> Minikube runs a single-node Kubernetes cluster inside a VM on your
> laptop for users looking to try out Kubernetes or develop with it day-to-day.

Before Minikube, running Kubernetes locally was pretty hard. You had some alternatives
like running a cluster using Vagrant or just use plain Docker.
Minikube gives you a much better experience in my opinion. Most of the setup is
pretty transparent for the user and like almost all of the Kubernetes tooling, it just works.

We'll use Minikube in order to test our Kubernetes configuration files
without having to launch test servers in the cloud. That's a very big step
in the container orchestration world. Even though Docker reduces the friction
between development and production environments, you still need to make the transition
between your local *deployment* and  your real production deployment.

Now, don't confuse testing your cluster locally with local development. Minikube isn't
yet a development tool, but a tool for testing your cluster's configuration before
shipping it to production. You still need to use Docker and Docker Compose or any
other tool you use daily to work on your code.

You can find the installation instructions for Minikube in the [official repository](https://github.com/kubernetes/minikube).
Check the releases page to find the instructions related to your operating system. 
In general, the installation should be pretty straightforward.

The version I have on my machine is v0.10.0 and I'll be using the `xhyve` driver.
You can use whatever is best for your platform.

You also need the `kubectl` CLI tool for launching the services and
deployments. [Here](http://kubernetes.io/docs/getting-started-guides/minikube/#download-kubectl) you can find the instructions to install `kubectl`.
This CLI tool communicates with our cluster in order to run containers and
services. It has a pretty solid integration with cloud services like Amazon AWS
and Google's GCE. This tool will be our main channel of interaction with our cluster.

Right now I'm using the version 1.4.0 for kubectl.
Make sure you're using that version or a newer one.

Once you have Minikube and kubectl ready to go, you can create your local
cluster by running (add the --vm-driver flag if you want to use an specific VM provider and
--kubernetes-version for an specific version of Kubernetes).

In my case I'm using MacOS with the xhyve drive and since I have the version 1.4.0
for my client (kubectl), I'll use the latest version available with Minikube for my cluster:

    $ minikube start --vm-driver=xhyve --kubernetes-version="v1.4.0-beta.10"

That can take a few minutes the first time. Once it finishes, you should see an output
similar to this:

    Starting local Kubernetes cluster...
    Kubectl is now configured to use the cluster.

That indicates that your local Kubernetes cluster is running. Kubectl will now be configured
to interact with your cluster. This is because Kubectl can have different contexts, which means
you can have different clusters configured at the same time and switch the context for
interacting with each one. In this case when we run `minikube start` it'll also
set the current context to minikube.

Minikube gives you a handful set of commands for interacting with the cluster.
For example you can run:

    $ minikube ip

Output:

    192.168.64.7

This is the IP address for your cluster.

You can also run:

    $ minikube dashboard

Which will open your default browser and take you to the Kubernetes UI
where you can see all the workloads running in the cluster. Right now we shouldn't
have any. You can also run tasks and services from this Dashboard. I don't think
many people would actually use the UI as a main way of managing their services, but
it's still a nice to have element for monitoring and have a quick view of what's happening in our
cluster.

### Running our templates with Minikube

Sadly, Kubernetes currently doesn't have a dependency mechanism like Docker Compose
does. So we have to create our resources in the correct order.

The first `kubectl` command you should know, is the `create -f` command. This command
can saves us a lot of headaches from trying to memorize different syntaxes for
running different Kubernetes kinds. You can pass any template as an argument
to this command, and as long as you have the `Kind` defined in the template, `kubectl`
will know what to do with it.

Let's start with the PostgreSQL deployment template.

Run the following command from the root of the application:

    $ kubectl create -f kube/deployments/postgres-deployment.yaml

Output:

    service "postgres" created
    deployment "postgres" created

We can describe our new deployment with:

    $ kubectl describe deployment postgres

Output:

    Name:			postgres
    Namespace:		default
    CreationTimestamp:	Mon, 26 Sep 2016 20:21:45 -0300
    Labels:			app=webapp
    Selector:		app=webapp,tier=postgres
    Replicas:		1 updated | 1 total | 1 available | 0 unavailable
    StrategyType:		Recreate
    MinReadySeconds:	0
    OldReplicaSets:		<none>
    NewReplicaSet:		postgres-1747921911 (1/1 replicas created)
    Events:
      FirstSeen	LastSeen	Count	From				SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----				-------------	--------	------			-------
      4s		4s		1	{deployment-controller }			Normal		ScalingReplicaSet	Scaled up replica set postgres-1747921911 to 1

If you run `minikube dashboard` again, you should see the new workloads under
the correspondent sections.

This deployment generates a service, a replica set and a Pod.
If you want to see the events for the Pod, you can run:

    $ kubectl describe Pod postgres

And you'll have a big output that describes the Pod specifications and also
the events in case something goes wrong. This is an output for the events section:

    Events:
      FirstSeen	LastSeen	Count	From			SubobjectPath			Type		Reason		Message
      ---------	--------	-----	----			-------------			--------	------		-------
      5m		5m		1	{default-scheduler }					Normal		Scheduled	Successfully assigned postgres-1747921911-h516t to minikube
      5m		5m		1	{kubelet minikube}	spec.containers{postgres}	Normal		Pulled		Container image "postgres:9.5.3" already present on machine
      5m		5m		1	{kubelet minikube}	spec.containers{postgres}	Normal		Created		Created container with docker id b1c664c98251; Security:[seccomp=unconfined]
      5m		5m		1	{kubelet minikube}	spec.containers{postgres}	Normal		Started		Started container with docker id b1c664c98251

As you can see, all the events are pretty standard. The image is pulled from DockerHub
and the container is launched using the specification we gave it.

Now that we have our database running, let's launch the setup job that's
going to run the migrations for the application:

    $ kubectl create -f kube/jobs/setup-job.yaml

Output:

    job "setup" created

This can take a few minutes the first time, since it has to pull the entire
image from DockerHub.

To see what's happening with our Job, we can filter the Job's Pods by the Job's name
and then run the logs command for that Pod.

First let's get the name of the Pod. The name we used for the job was `setup`:

    $ Pods=$(kubectl get Pods --selector=job-name=setup --output=jsonpath={.items..metadata.name})

Now if you run `echo $Pods` you'll get the name of the associated Pod.

Let's see the logs (if you see an error with ContainerCreating status, wait for a few
more minutes):

    $ kubectl logs $Pods

Output:

    Waiting PostgreSQL to start on 5432...
    PostgreSQL started
    D, [2016-09-26T23:29:37.704679 #6] DEBUG -- :    (103.0ms)  CREATE TABLE "schema_migrations" ("version" character varying PRIMARY KEY)
    D, [2016-09-26T23:29:37.776729 #6] DEBUG -- :    (67.1ms)  CREATE TABLE "ar_internal_metadata" ("key" character varying PRIMARY KEY, "value" character varying, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
    D, [2016-09-26T23:29:37.778284 #6] DEBUG -- :    (0.3ms)  SELECT pg_try_advisory_lock(1932410105524239390);
    D, [2016-09-26T23:29:37.822212 #6] DEBUG -- :   ActiveRecord::SchemaMigration Load (0.6ms)  SELECT "schema_migrations".* FROM "schema_migrations"
    I, [2016-09-26T23:29:37.839451 #6]  INFO -- : Migrating to CreateArticles (20160925220117)
    D, [2016-09-26T23:29:37.841086 #6] DEBUG -- :    (0.2ms)  BEGIN
    == 20160925220117 CreateArticles: migrating ===================================
    -- create_table(:articles)
    D, [2016-09-26T23:29:37.908021 #6] DEBUG -- :    (63.7ms)  CREATE TABLE "articles" ("id" serial primary key, "title" character varying, "body" text, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
       -> 0.0652s
    == 20160925220117 CreateArticles: migrated (0.0656s) ==========================

    D, [2016-09-26T23:29:37.914849 #6] DEBUG -- :   SQL (0.5ms)  INSERT INTO "schema_migrations" ("version") VALUES ($1) RETURNING "version"  [["version", "20160925220117"]]
    D, [2016-09-26T23:29:37.937591 #6] DEBUG -- :    (22.2ms)  COMMIT
    D, [2016-09-26T23:29:37.943480 #6] DEBUG -- :   ActiveRecord::InternalMetadata Load (0.3ms)  SELECT  "ar_internal_metadata".* FROM "ar_internal_metadata" WHERE "ar_internal_metadata"."key" = $1 LIMIT $2  [["key", :environment], ["LIMIT", 1]]
    D, [2016-09-26T23:29:37.947971 #6] DEBUG -- :    (0.1ms)  BEGIN
    D, [2016-09-26T23:29:37.951697 #6] DEBUG -- :   SQL (2.5ms)  INSERT INTO "ar_internal_metadata" ("key", "value", "created_at", "updated_at") VALUES ($1, $2, $3, $4) RETURNING "key"  [["key", "environment"], ["value", "development"], ["created_at", 2016-09-26 23:29:37 UTC], ["updated_at", 2016-09-26 23:29:37 UTC]]
    D, [2016-09-26T23:29:37.963675 #6] DEBUG -- :    (11.4ms)  COMMIT
    D, [2016-09-26T23:29:37.964502 #6] DEBUG -- :    (0.4ms)  SELECT pg_advisory_unlock(1932410105524239390)
    D, [2016-09-26T23:29:37.970691 #6] DEBUG -- :   ActiveRecord::SchemaMigration Load (0.2ms)  SELECT "schema_migrations".* FROM "schema_migrations"
    D, [2016-09-26T23:29:37.988811 #6] DEBUG -- :    (1.8ms)  SELECT t2.oid::regclass::text AS to_table, a1.attname AS column, a2.attname AS primary_key, c.conname AS name, c.confupdtype AS on_update, c.confdeltype AS on_delete
    FROM pg_constraint c
    JOIN pg_class t1 ON c.conrelid = t1.oid
    JOIN pg_class t2 ON c.confrelid = t2.oid
    JOIN pg_attribute a1 ON a1.attnum = c.conkey[1] AND a1.attrelid = t1.oid
    JOIN pg_attribute a2 ON a2.attnum = c.confkey[1] AND a2.attrelid = t2.oid
    JOIN pg_namespace t3 ON c.connamespace = t3.oid
    WHERE c.contype = 'f'
      AND t1.relname = 'articles'
      AND t3.nspname = ANY (current_schemas(false))
    ORDER BY c.conname

Cool! So the container ran the migrations and then died. Just what we wanted.
If you want to make sure the container died, you can list the Pods with:

    $ kubectl get Pods

Output:

    NAME                        READY     STATUS    RESTARTS   AGE
    postgres-1747921911-h516t   1/1       Running   0          10m
      info: 1 completed object(s) was(were) not shown in Pods list. Pass --show-all to see all objects.

Just the PostgreSQL Pod and 1 completed Pods, which was the migrate job.

Now, let's run the last piece, the web application:

    $ kubectl create -f kube/deployments/webapp-deployment.yaml

Output:

    service "webapp" created
    deployment "webapp" created

Don't worry if you get a warning about port security. That message would be relevant only if we were
using this service type on a production cluster.

This command should run much faster than the previous one, since we already have the image
on the cluster. Let's check that by running:

    $ kubectl get Pods

Output:

    NAME                        READY     STATUS    RESTARTS   AGE
    postgres-1747921911-h516t   1/1       Running   0          12m
    webapp-78833492-68glu       1/1       Running   0          53s
    webapp-78833492-op9v6       1/1       Running   0          53s
    webapp-78833492-riyfd       1/1       Running   0          53s
      info: 1 completed object(s) was(were) not shown in Pods list. Pass --show-all to see all objects.

There you can see the three replicas we declared for the web application running along the PostgreSQL Pod.

Let's check the logs for one of those Pods:

    $ kubectl logs webapp-78833492-68glu

Output:

    *** Running /etc/my_init.d/00_regen_ssh_host_keys.sh...
    *** Running /etc/my_init.d/30_presetup_nginx.sh...
    *** Running /etc/rc.local...
    *** Booting runit daemon...
    *** Runit started as PID 8
    ok: run: /etc/service/nginx-log-forwarder: (pid 27) 0s
    Sep 26 23:32:57 webapp-78833492-68glu syslog-ng[19]: syslog-ng starting up; version='3.5.6'
    [ 2016-09-26 23:32:58.9714 28/7f31c0c0b780 age/Wat/WatchdogMain.cpp:1291 ]: Starting Passenger watchdog...
    [ 2016-09-26 23:32:58.9989 31/7f7166c2d780 age/Cor/CoreMain.cpp:982 ]: Starting Passenger core...
    [ 2016-09-26 23:32:58.9990 31/7f7166c2d780 age/Cor/CoreMain.cpp:235 ]: Passenger core running in multi-application mode.
    [ 2016-09-26 23:32:59.0071 31/7f7166c2d780 age/Cor/CoreMain.cpp:732 ]: Passenger core online, PID 31
    [ 2016-09-26 23:32:59.0285 38/7f0462922780 age/Ust/UstRouterMain.cpp:529 ]: Starting Passenger UstRouter...
    [ 2016-09-26 23:32:59.0359 38/7f0462922780 age/Ust/UstRouterMain.cpp:342 ]: Passenger UstRouter online, PID 38

So it seems the passenger process started with no errors.

If we want to see our application in action, there's a very nice
Minikube command that will takes us to our service.
Run:

    $ minikube service webapp

And that should take you to the cluster's IP address and the port that
was assigned to our service. There's where our application lives. Every time you hit
that URL, the traffic gets load balanced among the replicas you declare for the service, which
in this case are 3 Pods.

Let's find our service address and make a post request to our
articles endpoint. First we need our Minikube VM IP:

    $ minikube ip

Output:

    192.168.64.7

And now the port the cluster assigned to our service:

    $ kubectl describe service webapp

Output:

    Name:			webapp
    Namespace:		default
    Labels:			app=webapp
    Selector:		app=webapp,tier=frontend
    Type:			NodePort
    IP:			10.0.0.122
    Port:			<unset>	80/TCP
    NodePort:		<unset>	30028/TCP
    Endpoints:		172.17.0.4:80,172.17.0.5:80,172.17.0.6:80
    Session Affinity:	None
    No events.%

So in my case I know that the service should be reachable at `http://192.168.64.7:30028`.
In your case, you have to find your `NodePort` in the service description in order
to find the exposed port.

Let's make a curl POST request to the endpoint:

    $ curl -H "Content-Type: application/json" -X POST -d '{"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit..."}' http://192.168.64.7:30028/articles

Output:

    {"id":1,"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit...","created_at":"2016-09-26T23:36:19.285Z","updated_at":"2016-09-26T23:36:19.285Z"}%

Great! We have our application fully running in our local Kubernetes cluster.

The transition to a production cluster will not be hard at all. We just need
to configure two more elements in our architecture, the Load Balancers and
the Volume for persisting the data.

There are also some minor changes we need to make to our
application to run on production. For example we'll need a secret token for production
and also configure our database.

## Launching an AWS Kubernetes Cluster

Right now if we run:

    $ kubectl config current-context

We get:

    minikube

That's because when we installed Minikube, an specific context was created
so we could interact with our local cluster with `kubectl`.

After we create our new AWS cluster, the Kubernetes installation script will create a new
context for us, so we can switch back and forth between Minikube and the production cluster.

The [getting started guide for AWS](http://kubernetes.io/docs/getting-started-guides/aws/)
indicates that we can run a simple command for launching a cluster with some
default settings. You have to keep in mind that these resources are not in the
free-tier, so you may want to delete the entire cluster once you don't need it
anymore. That way you only are going to pay accordingly to the amount of hours
your instances were up.

The documentation also indicates that:

> By default, the script will provision a new VPC and a 4 node k8s cluster in us-west-2a (Oregon) with EC2 instances running on Debian.

And then mentions that you can override this configuration by setting the following
environmental variables in your system:

    KUBE_AWS_ZONE
    NUM_NODES
    MASTER_SIZE
    NODE_SIZE
    AWS_S3_REGION
    AWS_S3_BUCKET
    INSTANCE_PREFIX

In our case four nodes can be a little too much just for this simple API
and the Database. Let's just use two small instances. In your profile file (.bashrc or .zshrc)
add the following:

    export NUM_NODES=2
    export NODE_SIZE=t2.small

And now source that file with (in my case):

    $ source ~/.zshrc

Now we can run the Kubernetes script that will launch a cluster
in our account:

    $ export KUBERNETES_PROVIDER=aws; curl -sS https://get.k8s.io | bash

That can take a while. The script will download Kubernetes (which is kind of heavy)
and then it'll launch the nodes and all the necessary elements for the cluster. It's
going to create Security Groups, a VPC, an S3 Bucket and the EC2 instances among
other stuff.

Once it's finished, at the end of the output you should see something like
this:

    Kubernetes master is running at https://52.32.34.173
    Elasticsearch is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
    Heapster is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/heapster
    Kibana is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/kibana-logging
    KubeDNS is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/kube-dns
    kubernetes-dashboard is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
    Grafana is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
    InfluxDB is running at https://52.32.34.173/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

There you can see all the different services that Kubernetes installed in our
cluster and you'll also find your cluster's IP address. It also has added the configuration credentials
for this cluster in the `~/.kube/config file`. There you should find a segment similar
to this:


      name: aws_kubernetes-basic-auth
      user:
        password: xxxxxxxxxxxxxxxx
        username: admin

If you inspect the file, you should also see the Minikube sections. This file
is what allows kubectl to interact with different cluster by switching context.
If you now run:

    $ kubectl config current-context
    aws_kubernetes

That's the new context Kubernetes created for us.

Visit your cluster's IP address and use those credentials for login into
the cluster.

Once you're logged in, you'll first see a list of endpoints you can
access from there. The dashboard is under the `/ui` namespace. So in my case
I can visit `https://52.32.34.173/ui` and that'll take me to my production
cluster's dashboard.

Now we just need to prepare our templates for production.

## Running the templates on production

First, we need to add a secret token for production environment.
In the root of the application, run:

    $ docker-compose run --rm webapp bin/rake secret RAILS_ENV=production

    Starting webapp_postgres_data_1
    Starting webapp_webapp_setup_1
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

And copy that token to the `config/secrets.yml` file under the production section.

    production:
      secret_key_base: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Now open the `config/database.yml` file and add the following values under the
production section:

    production:
      <<: *default
      host: postgres
      database: webapp_production
      username: webapp
      password: mysecretpassword

Notice here we are using a new database named `webapp_production`. Right now
we are declaring the DB credentials in the postgres deployment template.
Open the `kube/deployments/postgres.yaml` and change the environmental variables to:

    env:
    - name: POSTGRES_PASSWORD
      value: mysecretpassword
    - name: POSTGRES_USER
      value: webapp
    - name: POSTGRES_DB
      value: webapp_production

Let's run this first template:

    $ kubectl create -f kube/deployments/postgres-deployment.yaml

Output:

    service "postgres" created
    deployment "postgres" created

Let's describe our service:

    $ kubectl describe service postgres

Output:

    Name:			postgres
    Namespace:		default
    Labels:			app=webapp
    Selector:		app=webapp,tier=postgres
    Type:			ClusterIP
    IP:			None
    Port:			<unset>	5432/TCP
    Endpoints:		10.244.0.7:5432
    Session Affinity:	None
    No events.%

So it seems the service was correctly deployed.

Let's continue with some changes to the setup container job template.
First, we must change the `PASSENGER_APP_ENV` environmental variable so 
the Rails environment is production:

    env:
    - name: PASSENGER_APP_ENV
      value: production

Next, the `setup.sh` we currently have is migrating our development database, since
it doesn't have the `RAILS_ENV=production` declaration along the command. 
Let's generate a new setup file for production environment:

    $ touch setup.production.sh
    $ chmod +x setup.production.sh

And add the following:

    #!/bin/sh

    echo "Waiting PostgreSQL to start on 5432..."

    while ! nc -z postgres 5432; do
      sleep 0.1
    done

    echo "PostgreSQL started"

    bin/rails db:migrate RAILS_ENV=production

As you can see, the file is practically the same. The only distinction is that
we are using the production value for the `RAILS_ENV` variable.
Since we have a new entrypoint, let's add this new file to the entrypoint
of the setup job command:

    command: ["/bin/bash", "./setup.production.sh"]

Before running the setup template, let's update our Docker image
using the script `push.sh` we created previously:

    $ git add .
    $ git commit -m 'add production templates'
    $ ./push.sh

Output (truncated):

    Sending build context to Docker daemon 24.32 MB
    Step 1 : FROM phusion/passenger-ruby23:0.9.19
     ---> 6841e494987f
    Step 2 : ENV HOME /root
     ---> Using cache
     ---> e70985107acc
    Step 3 : CMD /sbin/my_init
     ---> Using cache
     ---> babd8b525225
    Step 4 : RUN apt-get update && apt-get install -y -o Dpkg::Options::="--force-confold" netcat
     ---> Using cache
     ---> 74da8c84b454
    Step 5 : RUN rm -f /etc/service/nginx/down
    ...
    The push refers to a repository [docker.io/pacuna/webapp]
    c3dc4ebe27b6: Pushed
    d29dfd2261fa: Pushing [================>                                  ] 7.566 MB/23.13 MB
    3f3247bf1fef: Pushed
    e5d0c4cf80b0: Pushing [================================>                  ] 15.17 MB/23.13 MB
    f7feb319b319: Layer already exists
    0945e5099009: Layer already exists
    90f2b6e5ebff: Layer already exists
    20138267dbc3: Layer already exists
    ...

Now that we have our image up-to-date, let's modify the setup job using the latest
tag we just pushed. You can get the latest tag running `git rev-parse --short HEAD` or
running `docker images` and see what is the latest generated image for your application:

    $ git rev-parse --short HEAD
    915685c

    $ docker images

    REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE
    pacuna/webapp              915685c             b0a8963a7a68        4 minutes ago       850.6 MB
    ...

So there I can see my latest tag is `915685c`.

Open the `kube/jobs/setup-job.yaml` and change the tag for the container image:

    containers:
    - name: setup
      image: pacuna/webapp:915685c

The final template should look like this:

    apiVersion: batch/v1
    kind: Job
    metadata:
      name: setup
    spec:
      template:
        metadata:
          name: setup
        spec:
          containers:
          - name: setup
            image: pacuna/webapp:915685c
            command: ["/bin/bash", "./setup.production.sh"]
            env:
            - name: PASSENGER_APP_ENV
              value: production
          restartPolicy: Never

And now we can run the template:

    $ kubectl create -f kube/jobs/setup-job.yaml

Output:

    job "setup" created

Check the logs after a few minutes to see if it's finished:

    $ Pods=$(kubectl get Pods --selector=job-name=setup --output=jsonpath={.items..metadata.name})
    $ kubectl logs $Pods

Output:

    Waiting PostgreSQL to start on 5432...
    PostgreSQL started
    D, [2016-09-27T02:22:06.207320 #6] DEBUG -- :    (21.4ms)  CREATE TABLE "schema_migrations" ("version" character varying PRIMARY KEY)
    D, [2016-09-27T02:22:06.218350 #6] DEBUG -- :    (8.0ms)  CREATE TABLE "ar_internal_metadata" ("key" character varying PRIMARY KEY, "value" character varying, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
    D, [2016-09-27T02:22:06.219207 #6] DEBUG -- :    (0.2ms)  SELECT pg_try_advisory_lock(3863469888790475145);
    D, [2016-09-27T02:22:06.227247 #6] DEBUG -- :   ActiveRecord::SchemaMigration Load (0.4ms)  SELECT "schema_migrations".* FROM "schema_migrations"
    I, [2016-09-27T02:22:06.237053 #6]  INFO -- : Migrating to CreateArticles (20160925220117)
    D, [2016-09-27T02:22:06.238575 #6] DEBUG -- :    (0.1ms)  BEGIN
    == 20160925220117 CreateArticles: migrating ===================================
    -- create_table(:articles)
    D, [2016-09-27T02:22:06.250670 #6] DEBUG -- :    (10.9ms)  CREATE TABLE "articles" ("id" serial primary key, "title" character varying, "body" text, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
       -> 0.0119s
    == 20160925220117 CreateArticles: migrated (0.0121s) ==========================

    D, [2016-09-27T02:22:06.255595 #6] DEBUG -- :   SQL (0.3ms)  INSERT INTO "schema_migrations" ("version") VALUES ($1) RETURNING "version"  [["version", "20160925220117"]]
    D, [2016-09-27T02:22:06.256840 #6] DEBUG -- :    (1.0ms)  COMMIT
    D, [2016-09-27T02:22:06.260468 #6] DEBUG -- :   ActiveRecord::InternalMetadata Load (0.3ms)  SELECT  "ar_internal_metadata".* FROM "ar_internal_metadata" WHERE "ar_internal_metadata"."key" = $1 LIMIT $2  [["key", :environment], ["LIMIT", 1]]
    D, [2016-09-27T02:22:06.263914 #6] DEBUG -- :    (0.1ms)  BEGIN
    D, [2016-09-27T02:22:06.265326 #6] DEBUG -- :   SQL (0.3ms)  INSERT INTO "ar_internal_metadata" ("key", "value", "created_at", "updated_at") VALUES ($1, $2, $3, $4) RETURNING "key"  [["key", "environment"], ["value", "production"], ["created_at", 2016-09-27 02:22:06 UTC], ["updated_at", 2016-09-27 02:22:06 UTC]]
    D, [2016-09-27T02:22:06.266304 #6] DEBUG -- :    (0.7ms)  COMMIT
    D, [2016-09-27T02:22:06.266731 #6] DEBUG -- :    (0.2ms)  SELECT pg_advisory_unlock(3863469888790475145)

So we can see the production database was correctly migrated.

Finally for the web application we want to make the same changes we did for the setup job
regards to the `PASSENGER_APP_ENV` and tag for the image, plus a small change. Until now
we have been using a `NodePort` type for the service. Well, now that we are on
production, we can use a `LoadBalancer` type, so we can have an static endpoint for that
service. Open the `kube/deployments/webapp-deployment.yaml` file and change
the service type to `LoadBalancer`:

    apiVersion: v1
    kind: Service
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      ports:
        - port: 80
      selector:
        app: webapp
        tier: frontend
      type: LoadBalancer

And finally change the tag image with your latest and also change the `PASSENGER_APP_ENV` to production.
The final deployment template should look like this:

    apiVersion: v1
    kind: Service
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      ports:
        - port: 80
      selector:
        app: webapp
        tier: frontend
      type: LoadBalancer
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: webapp
      labels:
        app: webapp
    spec:
      replicas: 3
      template:
        metadata:
          labels:
            app: webapp
            tier: frontend
        spec:
          containers:
          - image: pacuna/webapp:915685c
            name: webapp
            env:
            - name: PASSENGER_APP_ENV
              value: production
            ports:
            - containerPort: 80
              name: webapp
            imagePullPolicy: Always

Now can create the webapp deployment:

    $ kubectl create -f kube/deployments/webapp-deployment.yaml
    service "webapp" created
    deployment "webapp" created

If we inspect the service with:

    $ kubectl describe service webapp

We'll see:

    Name:                 webapp
    Namespace:            default
    Labels:               app=webapp
    Selector:             app=webapp,tier=frontend
    Type:                 LoadBalancer
    IP:                   10.0.250.219
    LoadBalancer Ingress: a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com
    Port:                 <unset>	80/TCP
    NodePort:             <unset>	30426/TCP
    Endpoints:            10.244.0.57:80,10.244.0.58:80,10.244.0.59:80
    Session Affinity:     None
    Events:
      FirstSeen	LastSeen	Count	From			SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----			-------------	--------	------			-------
      32s		32s		1	{service-controller }			Normal		CreatingLoadBalancer	Creating load balancer
      30s		30s		1	{service-controller }			Normal		CreatedLoadBalancer	Created load balancer


Kubernetes created an AWS Load Balancer for us and it's attached
to this service. The endpoint is in the `LoadBalancer Ingress` field and my case
corresponds to `a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com`.

We can also describe our deployment to see if our replicas are available:

    $ kubectl describe deployment webapp

Output:

    Name:                  webapp
    Namespace:             default
    CreationTimestamp:     Mon, 26 Sep 2016 23:29:22 -0300
    Labels:                app=webapp
    Selector:              app=webapp,tier=frontend
    Replicas:              3 updated | 3 total | 3 available | 0 unavailable
    StrategyType:          RollingUpdate
    MinReadySeconds:       0
    RollingUpdateStrategy: 1 max unavailable, 1 max surge
    OldReplicaSets:        <none>
    NewReplicaSet:         webapp-1367336656 (3/3 replicas created)
    Events:
      FirstSeen	LastSeen	Count	From				SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----				-------------	--------	------			-------
      2m		2m		1	{deployment-controller }			Normal		ScalingReplicaSet	Scaled up replica set webapp-1367336656 to 3

There you see we have actually 3 Pods running our application. The Load Balancer
associated to the service, will be balancing requests among this three replicas.
This integration with AWS is pretty good. We just had to put the `LoadBalancer`
type and Kubernetes automatically will create that resource in our account.
Keep in mind that this kind of integration is currently possible with only
a couple of cloud providers. Currently the best integrations are with Amazon AWS
and Google GCE.

You'll have to wait for a few seconds for that endpoint to be alive.
Once it's ready you can use the cURL post command to test it:

    $ curl -H "Content-Type: application/json" -X POST -d '{"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit..."}' http://a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com/articles

Output:

    {"id":1,"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit...","created_at":"2016-09-27T02:36:57.972Z","updated_at":"2016-09-27T02:36:57.972Z"}%

Great!, The endpoint is working correctly.

We have successfully deployed our Rails API in a Kubernetes cluster. Now
we are going to dig into more advanced topics such as Volumes for persisting data,
and running deployment updates to our application.

## Adding persistence

A very useful integration between Kubernetes and AWS consists in that you can
mount AWS Volumes in a very transparent way into your containers. We only need to create
a volume through the AWS API and then add a new section in the deployment
file for the service you want to persist. In our case we only need persistence
for the PostgreSQL container. With that in place, if the container gets killed, the data
will remain in the AWS Volume and can be read by the next container placed by the Kubernetes
scheduler.

Let's start by creating the volume. We can do this with a very simple command
using the AWS CLI. The only detail here is that you need to use the same region
your cluster lives in. If you didn't change the default one, then the region will be
`us-west-2` and you can use something like `us-west-2a` for the availability zone.
Remember that our default region may be different that the one Kubernetes used for creating the cluster, 
so let's also put it in the command along the availability zone:

    $ aws ec2 create-volume --region us-west-2 --availability-zone us-west-2a --size 10 --volume-type gp2

Output:

    {
        "AvailabilityZone": "us-west-2a",
        "Encrypted": false,
        "VolumeType": "gp2",
        "VolumeId": "vol-fe268f4a",
        "State": "creating",
        "Iops": 100,
        "SnapshotId": "",
        "CreateTime": "2016-09-27T02:51:23.429Z",
        "Size": 10
    }

The output gives us a `VolumeId` among several attributes for the created volume.
We're going to need that Volume Id later for our template so don't lose it.

Now, let's open the deployment file for the PostgreSQL service and change the
container specification so it looks like this:

    spec:
      containers:
      - image: postgres:9.5.3
        name: postgres
        env:
        - name: POSTGRES_PASSWORD
          value: mysecretpassword
        - name: POSTGRES_USER
          value: webapp
        - name: POSTGRES_DB
          value: webapp_production
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
          name: postgres
        volumeMounts:
          - name: postgres-persistent-storage
            mountPath: /var/lib/postgresql/data
      volumes:
        - name: postgres-persistent-storage
          awsElasticBlockStore:
            volumeID: vol-fe268f4a
            fsType: ext4

Replace the `volumeID` property with the one you got previously.

In order to use the AWS Volume, we have to declare a new `PGDATA` for the database, a volume, and also
mount that volume using the `volumeMounts` property. Then, by only using
its `VolumeId`, we can access the volume and save our data.
As you can see, we are using the native `awsElasticBlockStore` property from Kubernetes
which allows the integration with the EBS service from AWS. This kind of syntax
for working with volumes is very usual in container tools. First, you have a block that declares the properties
of the volume, and then you mount that volume into your container using the `volumeMounts`
in this case.

It's time to try out the Volume.
Let's delete our current deployment and launch a new one:

    $ kubectl delete -f kube/deployments/postgres-deployment.yaml

And let's run it again with the new changes:

    $ kubectl create -f kube/deployments/postgres-deployment.yaml

Output:

    service "postgres" created
    deployment "postgres" created

We can describe the postgres replica set to see if it is in fact using the
volume:

    $ kubectl describe rs postgres

Output:

    Name:		postgres-560075503
    Namespace:	default
    Image(s):	postgres:9.5.3
    Selector:	app=webapp,Pod-template-hash=560075503,tier=postgres
    Labels:		app=webapp
                    Pod-template-hash=560075503
                    tier=postgres
    Replicas:	1 current / 1 desired
    Pods Status:	1 Running / 0 Waiting / 0 Succeeded / 0 Failed
    Volumes:
      postgres-persistent-storage:
        Type:	AWSElasticBlockStore (a Persistent Disk resource in AWS)
        VolumeID:	vol-fe268f4a
        FSType:	ext4
        Partition:	0
        ReadOnly:	false
    Events:
      FirstSeen	LastSeen	Count	From				SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----				-------------	--------	------			-------
      28s		28s		1	{replicaset-controller }			Normal		SuccessfulCreate	Created Pod: postgres-560075503-z75yl

You can see there the Volumes section showing that the volume is correctly
mounted into the container.

We can also query the AWS API to see if can gives us some information of which instance
is this volume attached to it:

    $ aws ec2 describe-volumes --volume-ids vol-fe268f4a --region us-west-2

Output:

    {
        "Volumes": [
            {
                "AvailabilityZone": "us-west-2a",
                "Attachments": [
                    {
                        "AttachTime": "2016-09-27T03:01:03.000Z",
                        "InstanceId": "i-8f86bb97",
                        "VolumeId": "vol-fe268f4a",
                        "State": "attached",
                        "DeleteOnTermination": false,
                        "Device": "/dev/xvdba"
                    }
                ],
                "Encrypted": false,
                "VolumeType": "gp2",
                "VolumeId": "vol-fe268f4a",
                "State": "in-use",
                "Iops": 100,
                "SnapshotId": "",
                "CreateTime": "2016-09-27T02:51:23.429Z",
                "Size": 10
            }
        ]
    }

There you can see that this volume is in fact attached to an instance. That's
the instance that's holding the PostgreSQL container right now and that belongs to the 
Kubernetes cluster.

If we want to do our smoke test one more time to see if the database is working correctly, we
first have to rerun our setup Job. Remember that this container migrates
the database and right now we are starting with a fresh PostgreSQL container that
from now on will be able to persist its data.

    $ kubectl delete job/setup
    $ kubectl create -f kube/jobs/setup-job.yaml

As always let's check out the logs:

    $ Pods=$(kubectl get Pods --selector=job-name=setup --output=jsonpath={.items..metadata.name})
    $ kubectl logs $Pods

Output:

    Waiting PostgreSQL to start on 5432...
    PostgreSQL started
    D, [2016-09-27T03:06:16.395761 #6] DEBUG -- :    (10.2ms)  CREATE TABLE "schema_migrations" ("version" character varying PRIMARY KEY)
    D, [2016-09-27T03:06:16.406388 #6] DEBUG -- :    (7.7ms)  CREATE TABLE "ar_internal_metadata" ("key" character varying PRIMARY KEY, "value" character varying, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
    D, [2016-09-27T03:06:16.407140 #6] DEBUG -- :    (0.2ms)  SELECT pg_try_advisory_lock(3863469888790475145);
    D, [2016-09-27T03:06:16.414507 #6] DEBUG -- :   ActiveRecord::SchemaMigration Load (0.3ms)  SELECT "schema_migrations".* FROM "schema_migrations"
    I, [2016-09-27T03:06:16.419156 #6]  INFO -- : Migrating to CreateArticles (20160925220117)
    D, [2016-09-27T03:06:16.420605 #6] DEBUG -- :    (0.1ms)  BEGIN
    == 20160925220117 CreateArticles: migrating ===================================
    -- create_table(:articles)
    D, [2016-09-27T03:06:16.429336 #6] DEBUG -- :    (7.5ms)  CREATE TABLE "articles" ("id" serial primary key, "title" character varying, "body" text, "created_at" timestamp NOT NULL, "updated_at" timestamp NOT NULL)
       -> 0.0086s
    == 20160925220117 CreateArticles: migrated (0.0087s) ==========================

    D, [2016-09-27T03:06:16.433646 #6] DEBUG -- :   SQL (0.3ms)  INSERT INTO "schema_migrations" ("version") VALUES ($1) RETURNING "version"  [["version", "20160925220117"]]
    D, [2016-09-27T03:06:16.435490 #6] DEBUG -- :    (1.6ms)  COMMIT
    D, [2016-09-27T03:06:16.438876 #6] DEBUG -- :   ActiveRecord::InternalMetadata Load (0.3ms)  SELECT  "ar_internal_metadata".* FROM "ar_internal_metadata" WHERE "ar_internal_metadata"."key" = $1 LIMIT $2  [["key", :environment], ["LIMIT", 1]]
    D, [2016-09-27T03:06:16.442136 #6] DEBUG -- :    (0.1ms)  BEGIN
    D, [2016-09-27T03:06:16.443314 #6] DEBUG -- :   SQL (0.3ms)  INSERT INTO "ar_internal_metadata" ("key", "value", "created_at", "updated_at") VALUES ($1, $2, $3, $4) RETURNING "key"  [["key", "environment"], ["value", "production"], ["created_at", 2016-09-27 03:06:16 UTC], ["updated_at", 2016-09-27 03:06:16 UTC]]
    D, [2016-09-27T03:06:16.444709 #6] DEBUG -- :    (1.1ms)  COMMIT
    D, [2016-09-27T03:06:16.445132 #6] DEBUG -- :    (0.2ms)  SELECT pg_advisory_unlock(3863469888790475145)

We are good to go. Now let's run that test using our ELB DNS address:

    $ curl -H "Content-Type: application/json" -X POST -d '{"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit..."}' http://a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com/articles

Output:

    {"id":1,"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit...","created_at":"2016-09-27T03:07:36.706Z","updated_at":"2016-09-27T03:07:36.706Z"}%

Great! It's working. But, how can we know that we are actually persisting the data?
Well, there's only one way to know. Let's delete our PostgreSQL deployment and run it
again one more time. This time we should not even need to run the setup, because
the tables should still be there. Also we have one article there, so let's hope it's
still there after a new deployment:

    $ kubectl delete -f kube/deployments/postgres-deployment.yaml

Output:

    service "postgres" deleted
    deployment "postgres" deleted

Now let's recreate it:

    $ kubectl create -f kube/deployments/postgres-deployment.yaml

Output:

    service "postgres" created
    deployment "postgres" created

Let's check if we have a valid response and our first article back:

    $ curl http://a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com/articles

Output:

    [{"id":1,"title":"my first article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit...","created_at":"2016-09-27T03:07:36.706Z","updated_at":"2016-09-27T03:07:36.706Z"}]%

Awesome. This means we can kill the PostgreSQL container and the data
won't be lost. You can apply this trick for persisting any type of data, for example
for a search engine like Elasticsearch or a different database like MySQL or MongoDB.

## Updating the application

One of the cool features of deployments in Kubernetes, is that you can
run updates very easily. You only need to create the new version
of the image you want to deploy, and then update the respective deployment templates.
After that  you can have a new version running with just one command.

We are going to add a new field to our articles table. This will require to apply migrations, so we also
have to run the setup container before the update. Don't worry, once we
start to use jenkins for our deployments, everything will be automatic. But for now
let just stay with the `kubectl` commands and some bash scripts.

Let's add the new field in our Rails application:

    $ docker-compose run --rm webapp bin/rails g migration AddSlugToArticles slug:string

Let's also generate this slug automatically before the record
gets saved in the articles#create action:

      def create
        @article = Article.new(article_params)
        @article.slug = @article.title.parameterize

        if @article.save
          render json: @article, status: :created, location: @article
        else
          render json: @article.errors, status: :unprocessable_entity
        end
      end

OK, I'm pretty sure that's going to work. Let's rebuild the image and push it
to DockerHub. Let's commit our changes so we have a new commit hash to tag
the Docker image and then use our push script:

    $ git add -A
    $ git commit -m 'add slug field to articles'
    $ ./push.sh

Now that we updated the image, we can change the tag in both the `setup-job` template and the
webapp-deployment templates to our latest commit hash:

    $ git rev-parse --short HEAD
    d15587c

And change the respective line for each file:

    image: pacuna/webapp:d15587c

Now, let's rerun our setup Job so the migrations are applied:

    $ kubectl delete jobs/setup
    $ kubectl create -f kube/jobs/setup-job.yaml

Check the logs:

    $ Pods=$(kubectl get Pods --selector=job-name=setup --output=jsonpath={.items..metadata.name})
    $ kubectl logs $Pods

Output:

    Waiting PostgreSQL to start on 5432...
    PostgreSQL started
    D, [2016-09-28T03:16:05.360263 #6] DEBUG -- :    (1.0ms)  SELECT pg_try_advisory_lock(3863469888790475145);
    D, [2016-09-28T03:16:05.369135 #6] DEBUG -- :   ActiveRecord::SchemaMigration Load (1.8ms)  SELECT "schema_migrations".* FROM "schema_migrations"
    I, [2016-09-28T03:16:05.374952 #6]  INFO -- : Migrating to AddSlugToArticles (20160928030155)
    D, [2016-09-28T03:16:05.376440 #6] DEBUG -- :    (0.1ms)  BEGIN
    == 20160928030155 AddSlugToArticles: migrating ================================
    -- add_column(:articles, :slug, :string)
    D, [2016-09-28T03:16:05.385134 #6] DEBUG -- :    (8.2ms)  ALTER TABLE "articles" ADD "slug" character varying
       -> 0.0085s
    == 20160928030155 AddSlugToArticles: migrated (0.0087s) =======================

    D, [2016-09-28T03:16:05.388399 #6] DEBUG -- :   SQL (0.3ms)  INSERT INTO "schema_migrations" ("version") VALUES ($1) RETURNING "version"  [["version", "20160928030155"]]
    D, [2016-09-28T03:16:05.390099 #6] DEBUG -- :    (1.4ms)  COMMIT
    D, [2016-09-28T03:16:05.395324 #6] DEBUG -- :   ActiveRecord::InternalMetadata Load (1.7ms)  SELECT  "ar_internal_metadata".* FROM "ar_internal_metadata" WHERE "ar_internal_metadata"."key" = $1 LIMIT $2  [["key", :environment], ["LIMIT", 1]]
    D, [2016-09-28T03:16:05.398654 #6] DEBUG -- :    (0.1ms)  BEGIN
    D, [2016-09-28T03:16:05.399649 #6] DEBUG -- :    (0.1ms)  COMMIT
    D, [2016-09-28T03:16:05.400064 #6] DEBUG -- :    (0.2ms)  SELECT pg_advisory_unlock(3863469888790475145)

So it worked correctly. You can see the new migration ran correctly.

And now to update our deployment we can run:

    $ kubectl apply -f kube/deployments/webapp-deployment.yaml

Output:

    service "webapp" configured
    deployment "webapp" configured

The `apply` command will check the differences between the current deployment and the 
passed template and it'll update the correspondent objects. In our case, the container
changed so it has to update the Pods. This command will be very helpful for automate
this task, since we just need to replace the image tag with the new version and the
run the command.

Wait for a second for the deployment to get updated and let's try to create a new
article:

    $ curl -H "Content-Type: application/json" -X POST -d '{"title":"my second article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit..."}' http://a333dae17845a11e6b47b06103f11903-585648094.us-west-2.elb.amazonaws.com/articles

Output:

    {"id":34,"title":"my second article","body":"Lorem ipsum dolor sit amet, consectetur adipiscing elit...","created_at":"2016-09-28T03:20:37.427Z","updated_at":"2016-09-28T03:20:37.427Z","slug":"my-second-article"}%

The slug was generated correctly, which means our migration was correctly applied and also
the code added to the articles controller is working.

We just made our first real deployment to our application. This new concept
of deployments allows us to run updates to our software very easily. You only need
to generate a new image, and update your current deployment by keeping it's metadata
and the using the `apply command`. Then Kubernetes will update the Pods, replica sets
and it'll make sure the service can route the requests to the correct Pods.

In the next section we'll see how to automate this entire process
from your local environment. Although this could be a very good stopping point for
most of the applications, we want to go further and build a solid Continuous Integration pipeline
with Jenkins. But first, let's work on some scripts for automating the migrations
and the deployments.

## Automation scripts

Automating the deployment is actually pretty easy. We just have to combine our
current push.sh script with a new command that's going to set a new image in our current
deployment. This command looks like this:

    $ kubectl set image deployment my-deployment container=image:new-tag

As you can see, we only need the name of our deployment, the container we want to update, and
the new image the container should be built from. Normally this would be the same original image 
but with a different tag. In our case the latest commit hash.

Let's create a new folder for this new deployment script and move the push.sh script inside of it:

    $ mkdir deploy
    $ mv push.sh deploy

Now, edit the deploy/push.sh file and add the following, replacing of course
with your DockerHub repository:

    #!/bin/sh
    set -x

    LC=$(git rev-parse --short HEAD)
    docker build -f Dockerfile -t pacuna/webapp:${LC} .
    docker push pacuna/webapp:${LC}
    kubectl set image deployment webapp webapp=pacuna/webapp:${LC}

This is almost the same push script but we added the `kubectl set` command
to update our current deployment.

For the migrate job it's a little bit more complicated. Currently there's not an easy
way of updating a job image and then re-run it. We first have to delete the current job, then
change the image in the Job template, and finally run it again.
The first part is actually pretty easy, for deleting the job we can just use:

    $ kubectl delete jobs/setup

Then for replacing the image with the latest tag, which we know is going to be
the latest commit, we can add a placeholder instead a real image tag in the file and
then use a tool like `sed` to replace it with the real tag.
Open the `kube/jobs/setup-job.yaml` and change the image to:

    image: pacuna/webapp:LAST_COMMIT

Then with `sed`, we can find that placeholder, create a new file where the placeholder
is replaced with the latest commit, run the `kubectl apply` with that temp file, and finally
delete it. Let's create the script and add those instructions:

    $ touch deploy/migrate.sh
    $ chmod +x deploy/migrate.sh

And add:

    #!/bin/sh
    set -x

    # get latest commit hash
    LC=$(git rev-parse --short HEAD)

    # delete current migrate job
    kubectl delete jobs/setup || true

    # replace LAST_COMMIT with latest commit hash output the result to a tmp file
    sed "s/webapp:LAST_COMMIT/webapp:$LC/g" kube/jobs/setup-job.yaml > setup-job.yaml.tmp

    # run the updated tmp job in the cluster and then delete the file
    kubectl create -f setup-job.yaml.tmp &&
      rm setup-job.yaml.tmp

And that's it! Now if you want to make changes to your code and deploy a new version, you just
have to commit your changes and the just run `deploy/push.sh`. If you want run migrations, you can
just run `deploy/migrate.sh`. You can even built another script that uses both for every
deployment, which is what we will later do with Jenkins.
