# ECS

## Configuring the ecs-cli

Currently there are two ways of creating an ECS cluster. You can use
the graphical interface from the AWS panel for ECS or you can make use
of the [esc-cli](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_reference.html)
tool. We are going to focus on the latter for launching the cluster.

By using any of these choices you'll get the same result which can be a little more
tedious if you decide to configure a cluster by yourself. You'll need to launch
instances with initial user data, security groups among other configuration.

The esc-cli is pretty easy to install. You can find more details in the
[official documentation](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html),
but basically you'll need to run one command.

The version I'm currently running is:

    $ ecs-cli --version

Output:

    ecs-cli version 0.4.4 (7e1376e)

If you have the AWS CLI tool already configured, meaning you have your
AWS access keys configured for the client, you can make use of the ecs-cli tool
immediately. If not, you can follow the instructions provided [here](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration).


## Creating the cluster using the Amazon ECS CLI

Before we launch the cluster, we'll need a key pair for access. You can
use an existent key or create a new one with the following command (change the profile
with yours or leave it blank if you're using the default profile):

    $ aws ec2 --profile personal --region us-east-1 create-key-pair --key-name ecs --query 'KeyMaterial' --output text > ecs.pem

Now we can use the configure command to add our future cluster information. This
command will save the configuration in a file so we don't have to specify all the data
when running future commands:

    $ ecs-cli configure --profile personal --region us-east-1 --cluster my-cluster

Output:

    INFO[0000] Saved ECS CLI configuration for cluster (my-cluster)

The configuration file is located at '~/.ecs/config'


Now that we have the configuration ready for the cluster, we can launch it along
with some nodes. Let's use two t2.medium nodes. We only need to pass the keypair we
just created and the number and size of the nodes:

    $ ecs-cli up --keypair ecs --capability-iam --size 2 --instance-type t2.medium

Output:

    INFO[0002] Created cluster                               cluster=my-cluster
    INFO[0004] Waiting for your cluster resources to be created
    INFO[0004] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS
    INFO[0066] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS
    INFO[0128] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS
    INFO[0190] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS
    INFO[0252] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS

This can take a while, and even after the command finishes, you'll have to wait
the instances to be initialized so they can join the cluster.

Let's use the AWS CLI to ask about our cluster:

    $ aws ecs --profile personal --region us-east-1 describe-clusters

Output:


    {
        "failures": [],
        "clusters": [
            {
                "pendingTasksCount": 0,
                "clusterArn": "arn:aws:ecs:us-east-1:586421825777:cluster/default",
                "status": "INACTIVE",
                "activeServicesCount": 0,
                "registeredContainerInstancesCount": 0,
                "clusterName": "default",
                "runningTasksCount": 0
            }
        ]
    }

There you can see the cluster I just launched was actually created but it doesn't have
any instance instance registered yet. That's because those are still initializing.

Wait for a few minutes, and let's run another command in order to get information about
that specific cluster we just launched:

    $ aws ecs --profile personal --region us-east-1 describe-clusters --clusters my-cluster

Output:

    {
        "failures": [],
        "clusters": [
            {
                "clusterName": "my-cluster",
                "status": "ACTIVE",
                "clusterArn": "arn:aws:ecs:us-east-1:586421825777:cluster/my-cluster",
                "registeredContainerInstancesCount": 2,
                "runningTasksCount": 0,
                "pendingTasksCount": 0,
                "activeServicesCount": 0
            }
        ]
    }

OK, that's better. Now you can see the cluster has a status of active and it has
two instances registered.

## DB configuration

AWS ECS doesn't currently has native support for DNS discovery like Kubernetes
has.

This is a big issue when you want to run a lot of different services in the cluster
and even bigger if lots of those service need to be non-public. This is because
the typical form for service discovery in ECS is via Load Balancer. So every time
you create a service that you need to be discoverable by other services, you
need to create a Load Balancer for it. This can be expensive if you're running
a micro-service arquitecture.

Another problem with ECS is that there's no support for associating cloud storage
to the instances. You can mount volumes from the containers to the instance, but
that doesn't work if your container will be jumping from one instance to another, which
is what typically occurs during deployments.

This two reasons are why I recommend you to not use ECS for running containers
that need persistence.

Luckily for us, when don't have to mount a server with a database engine
and configure the whole thing, since AWS has a very good service call RDS.
This service will allow us to run a new database server already configured
and that we can use to connect to our web application that will be running
on a container.

### Creating a RDS resource

(show how to get the vpc id of the cluster along the subnet ids)

- Creating a DB subnet group

    $ aws rds create-db-subnet-group --db-subnet-group-name postgres-subnet \
    --subnet-ids subnet-dd3ecef0 subnet-8c8b7dd7 --profile personal \
    --region us-east-1 --db-subnet-group-description "Subnet for PostgreSQL"


Output:

    {
        "DBSubnetGroup": {
            "VpcId": "vpc-934161f4",
            "DBSubnetGroupDescription": "Subnet for PostgreSQL",
            "Subnets": [
                {
                    "SubnetStatus": "Active",
                    "SubnetIdentifier": "subnet-dd3ecef0",
                    "SubnetAvailabilityZone": {
                        "Name": "us-east-1c"
                    }
                },
                {
                    "SubnetStatus": "Active",
                    "SubnetIdentifier": "subnet-8c8b7dd7",
                    "SubnetAvailabilityZone": {
                        "Name": "us-east-1a"
                    }
                }
            ],
            "SubnetGroupStatus": "Complete",
            "DBSubnetGroupName": "postgres-subnet",
            "DBSubnetGroupArn": "arn:aws:rds:us-east-1:586421825777:subgrp:postgres-subnet"
        }
    }


Now we can use that db subnet group to create our database:

    $ aws rds create-db-instance --region us-east-1 --db-name articles --db-instance-identifier articles-db \
    --allocated-storage 20 --db-instance-class db.t2.medium --engine postgres \
    --master-username articles --master-user-password mysecretpassword --db-subnet-group-name postgres-subnet \
    --vpc-security-group-ids sg-daf48fa0


Output:

    {
        "DBInstance": {
            "PubliclyAccessible": false,
            "MasterUsername": "articles",
            "MonitoringInterval": 0,
            "LicenseModel": "postgresql-license",
            "VpcSecurityGroups": [
                {
                    "Status": "active",
                    "VpcSecurityGroupId": "sg-daf48fa0"
                }
            ],
            "CopyTagsToSnapshot": false,
            "OptionGroupMemberships": [
                {
                    "Status": "in-sync",
                    "OptionGroupName": "default:postgres-9-5"
                }
            ],
            "PendingModifiedValues": {
                "MasterUserPassword": "****"
            },
            "Engine": "postgres",
            "MultiAZ": false,
            "DBSecurityGroups": [],
            "DBParameterGroups": [
                {
                    "DBParameterGroupName": "default.postgres9.5",
                    "ParameterApplyStatus": "in-sync"
                }
            ],
            "AutoMinorVersionUpgrade": true,
            "PreferredBackupWindow": "05:52-06:22",
            "DBSubnetGroup": {
                "Subnets": [
                    {
                        "SubnetStatus": "Active",
                        "SubnetIdentifier": "subnet-dd3ecef0",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1c"
                        }
                    },
                    {
                        "SubnetStatus": "Active",
                        "SubnetIdentifier": "subnet-8c8b7dd7",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1a"
                        }
                    }
                ],
                "DBSubnetGroupName": "postgres-subnet",
                "VpcId": "vpc-934161f4",
                "DBSubnetGroupDescription": "Subnet for PostgreSQL",
                "SubnetGroupStatus": "Complete"
            },
            "ReadReplicaDBInstanceIdentifiers": [],
            "AllocatedStorage": 20,
            "BackupRetentionPeriod": 1,
            "DBName": "articles",
            "PreferredMaintenanceWindow": "sat:06:41-sat:07:11",
            "DBInstanceStatus": "creating",
            "EngineVersion": "9.5.2",
            "DomainMemberships": [],
            "StorageType": "standard",
            "DbiResourceId": "db-JD5W6CCXXDLHIPJMWSMWWAFKSI",
            "CACertificateIdentifier": "rds-ca-2015",
            "StorageEncrypted": false,
            "DBInstanceClass": "db.t2.medium",
            "DbInstancePort": 0,
            "DBInstanceIdentifier": "articles-db"
        }
    }


You can query the API to get the current DB Instance status with the following command:

    $ aws rds describe-db-instances --db-instance-identifier articles-db --query 'DBInstances[*].{Status:DBInstanceStatus}'

    [
        {
            "Status": "creating"
        }
    ]

Wait for a few minutes until the DB is ready and the status becomes available:

    [
        {
            "Status": "available"
        }
    ]

Once the server is available, we can query the endpoint AWS gave to the DB:

    $ aws rds describe-db-instances --db-instance-identifier articles-db --query 'DBInstances[*].{URL:Endpoint.Address}'

Output:

    [
        {
            "URL": "articles-db.caxygd3nh0bk.us-east-1.rds.amazonaws.com"
        }
    ]

Great! Now we have our database ready. But before configure it in our application we have
to do one thing.
Right now we are using the cluster's security group. Currently this group
should only have an inbound rule for the port 80. What we want is to allow
free traffic between elements that live inside this VPC.

First, let's list our existent security groups with their names and ids so we can
find the one that ECS created for us:

    $ aws ec2 describe-security-groups --query="SecurityGroups[*].{Name:GroupName,ID:GroupId}"

Output:

    [
        {
            "Name": "default",
            "ID": "sg-0132736a"
        },
        {
            "Name": "amazon-ecs-cli-setup-my-cluster-EcsSecurityGroup-197TU7Y7R76ZV",
            "ID": "sg-daf48fa0"
        },
        {
            "Name": "default",
            "ID": "sg-3cf48f46"
        },
        {
            "Name": "default",
            "ID": "sg-5291fa35"
        }
    ]

As you can see, I have a couple of duplicated default security groups but there
in between you can see the cluster security group and you should see it two in
your output. Let's grab that id and create a new ingress rule that's going to
allow all traffic from inside that same group.

We can create a custom rule for this with the following command:

    $ aws ec2 authorize-security-group-ingress --group-id sg-daf48fa0 --protocol all --port all --source-group sg-daf48fa0

Let's also add a rule for accessing the nodes via ssh from anywhere. This can be useful
for debugging and diagnose container errors:

    $ aws ec2 authorize-security-group-ingress --group-id sg-daf48fa0 --protocol tcp --port 22 --cidr 0.0.0.0/0


Now we can configure the production database credentials in our application:

    production:
      <<: *default
      host: articles-db.caxygd3nh0bk.us-east-1.rds.amazonaws.com
      database: articles
      username: articles
      password: mysecretpassword

Let's rebuild our Docker image and add a tag for ECS:

    $ docker build -t pacuna/articles:ecs-v_0 .

And push it to DockerHub:

    $ docker push pacuna/articles:ecs-v_0

Once it finishes, let's create an skeleton task definition for our application with the following command:

    $ aws ecs register-task-definition --generate-cli-skeleton > articles.json

That's going to create an empty task definition we can fill out with our data.

Let's clean up the file a little bit and the information for our first task definition.
Open the file and replace its content with:

    {
        "family": "articles",
        "containerDefinitions": [
            {
                "name": "articles",
                "image": "pacuna/articles:ecs-v_0",
                "cpu": 500,
                "memory": 500,
                "portMappings": [
                    {
                        "containerPort": 80,
                        "hostPort": 80,
                        "protocol": "tcp"
                    }
                ],
                "essential": true,
                "environment": [
                    {
                        "name": "PASSENGER_APP_ENV",
                        "value": "production"
                    }
                ]
            }
        ]
    }

Now we can register this file with:

    $ aws ecs register-task-definition --cli-input-json file://ecs/task-definitions/articles.json

Output:

    {
        "taskDefinition": {
            "status": "ACTIVE",
            "family": "articles",
            "volumes": [],
            "taskDefinitionArn": "arn:aws:ecs:us-east-1:586421825777:task-definition/articles:5",
            "containerDefinitions": [
                {
                    "environment": [
                        {
                            "name": "PASSENGER_APP_ENV",
                            "value": "production"
                        }
                    ],
                    "name": "articles",
                    "mountPoints": [],
                    "image": "pacuna/articles:ecs-v_0",
                    "cpu": 500,
                    "portMappings": [
                        {
                            "protocol": "tcp",
                            "containerPort": 80,
                            "hostPort": 80
                        }
                    ],
                    "memory": 500,
                    "essential": true,
                    "volumesFrom": []
                }
            ],
            "revision": 5
        }
    }

The importan part of the output is the family name which we declared in the json file,
and the revision number. Every time we modify a task definition we create a new
revision. In this case I got revision 5 because I've been playing around with this
task.

We can run this task with:

    $ aws ecs run-task --task-definition articles:5 --cluster my-cluster

Output:

    {
        "failures": [],
        "tasks": [
            {
                "taskArn": "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82",
                "overrides": {
                    "containerOverrides": [
                        {
                            "name": "articles"
                        }
                    ]
                },
                "lastStatus": "PENDING",
                "containerInstanceArn": "arn:aws:ecs:us-east-1:586421825777:container-instance/02a39ed9-9364-4449-969d-e19762013a24",
                "createdAt": 1474745245.239,
                "clusterArn": "arn:aws:ecs:us-east-1:586421825777:cluster/my-cluster",
                "desiredStatus": "RUNNING",
                "taskDefinitionArn": "arn:aws:ecs:us-east-1:586421825777:task-definition/articles:5",
                "containers": [
                    {
                        "containerArn": "arn:aws:ecs:us-east-1:586421825777:container/3fb816c9-a9b2-400e-a104-4bb99e20ad84",
                        "taskArn": "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82",
                        "lastStatus": "PENDING",
                        "name": "articles"
                    }
                ]
            }
        ]
    }

That command is going to run our container somewhere in the cluster. If we want
to get more info about our task, first we have to get its identifier:

    $ aws ecs list-tasks --cluster my-cluster

Output:

    {
        "taskArns": [
            "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82"
        ]
    }

Now we can use that identifier in order we what we want:

    $ aws ecs describe-tasks --tasks arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82 --cluster my-cluster

Output:

    {
        "failures": [],
        "tasks": [
            {
                "taskArn": "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82",
                "overrides": {
                    "containerOverrides": [
                        {
                            "name": "articles"
                        }
                    ]
                },
                "lastStatus": "RUNNING",
                "containerInstanceArn": "arn:aws:ecs:us-east-1:586421825777:container-instance/02a39ed9-9364-4449-969d-e19762013a24",
                "createdAt": 1474745245.239,
                "clusterArn": "arn:aws:ecs:us-east-1:586421825777:cluster/my-cluster",
                "startedAt": 1474745246.911,
                "desiredStatus": "RUNNING",
                "taskDefinitionArn": "arn:aws:ecs:us-east-1:586421825777:task-definition/articles:5",
                "containers": [
                    {
                        "containerArn": "arn:aws:ecs:us-east-1:586421825777:container/3fb816c9-a9b2-400e-a104-4bb99e20ad84",
                        "taskArn": "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82",
                        "lastStatus": "RUNNING",
                        "name": "articles",
                        "networkBindings": [
                            {
                                "protocol": "tcp",
                                "bindIP": "0.0.0.0",
                                "containerPort": 80,
                                "hostPort": 80
                            }
                        ]
                    }
                ]
            }
        ]
    }

We can see the last status was running. If in your case it says pending, just wait
until the image gets pulled and the container starts.

Now, we know our application is not going to work out of the box. We have our database
created but we haven't run the migrations yet.
In Kubernetes we used the concept of jobs for running this kind of stuff. In this case
we are going to run a task that's going to override a task definition. This means
we can run this same task but with an override for its entrypoint, which is what we need.

Let's create a new json file that's going to contain the overrides for the migrations:

    $ touch ecs/task-definitions/migrate-overrides.json

And add the following to it:

    {
      "containerOverrides": [
        {
          "name": "articles",
          "command": ["bin/rails", "db:migrate", "RAILS_ENV=production"],
          "environment": [
            {
              "name": "PASSENGER_APP_ENV",
              "value": "production"
            }
          ]
        }
      ]
    }

As you can see, there's nothing crazy going on there. We are just setting a new
entrypoint and making sure the environmental variables are kept.

In order to run a task with this overrides we can run the following command:

    $ aws ecs run-task --task-definition articles:5 --overrides file://ecs/task-definitions/migrate-overrides.json --cluster my-cluster

That's going to take our same task definition, overrides its entrypoint so the migrations
are executed, and then the task will be stopped.

Now we just need to get the IP address of the node that's running our application.
This can be a little bit tedious to do using the AWS CLI, but don't worry. Later we'll
create a service with an associated Load Balancer that's going to give us a static
DNS for our container.

This are the steps to get the IP address of the node that's running the task:

1) Get our task identifier:

    $ aws ecs list-tasks --cluster my-cluster

    {
        "taskArns": [
            "arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82"
        ]
    }

2) Describe the task with that identifier filter by the container instance identifier:

    $ aws ecs describe-tasks --tasks arn:aws:ecs:us-east-1:586421825777:task/a981b740-8d0d-4c3d-88af-4cf4d415de82 --cluster my-cluster --query="tasks[*].containerInstanceArn"

    [
        "arn:aws:ecs:us-east-1:586421825777:container-instance/02a39ed9-9364-4449-969d-e19762013a24"
    ]

3) Using the instance identifier, get the instance id:

    $ aws ecs describe-container-instances --container-instances arn:aws:ecs:us-east-1:586421825777:container-instance/02a39ed9-9364-4449-969d-e19762013a24 --query="containerInstances[*].ec2InstanceId" --cluster my-cluster

    [
        "i-21762a10"
    ]

4) Using that id, get the IP address by using the EC2 API:

    $ aws ec2 describe-instances --instance-ids i-21762a10 --query="Reservations[0].Instances[0].PublicIpAddress"

    "54.196.162.145"

Now, let's CURL that ip address to see if it's actually responding:

    $ curl -I http://54.196.162.145/articles

    HTTP/1.1 200 OK
    Content-Type: application/json; charset=utf-8
    Connection: keep-alive
    Status: 200 OK
    Cache-Control: max-age=0, private, must-revalidate
    ETag: W/"4f53cda18c2baa0c0354bb5f9a3ecbe5"
    X-Frame-Options: SAMEORIGIN
    X-XSS-Protection: 1; mode=block
    X-Content-Type-Options: nosniff
    X-Runtime: 0.002354
    X-Request-Id: 2086674f-9d72-4d27-9519-e3a499c440e0
    Date: Sat, 24 Sep 2016 19:57:55 GMT
    X-Powered-By: Phusion Passenger 5.0.29
    Server: nginx/1.10.1 + Phusion Passenger 5.0.29

Great! Our application is up and running with no errors.
In the next section we'll create a service that's going to keep our task alive
24/7 and also create a Load Balancer so we can have a static address for our
application.
